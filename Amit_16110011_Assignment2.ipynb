{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amit_16110011_Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksy1999/NLP_Assignment_2/blob/master/Amit_16110011_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbZfUNRMWRM_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "02b02690-588d-48e4-c0ef-152d232da562"
      },
      "source": [
        "#16110011 -Amit Kumar Singh Yadav, \n",
        "#TASK 2: NEURAL APPROACH LANGUAGE MODELLING | COllaborators - TA's (USE CODE GIVEN IN DEEP LEARNING CODING SESSION)\n",
        "#Definng all headers \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import random\n",
        "import collections\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tensorflow.contrib import rnn\n",
        "from __future__ import print_function\n",
        "from operator import itemgetter\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from string import punctuation\n",
        "from nltk.util import ngrams\n",
        "from sklearn.model_selection import train_test_split \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mWk36_zK7bI",
        "colab_type": "code",
        "outputId": "ae995bb3-ddd3-4bde-fac0-0b59f6520fdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Pre-Processing Dataset -Filtering out symbols, newlines. \n",
        "#Parsing and dividing in 80-20 ratio\n",
        "\n",
        "#CODE SIMILAR TO PROCESSING DONE IN 1st part using Language Model\n",
        "# DATASET- Jane Austen Novels: The Complete Works of Jane Austen \n",
        "#take one line, neglect empty line and then add it to line_set then lower down whole set(string)\n",
        "with open(\"31100.txt\", 'rt') as source_file:  \n",
        "    line_set = ['']\n",
        "    for line in (l.rstrip() for l in source_file):\n",
        "        if line != '' or line_set[-1] !='\\n':                 \n",
        "            line_set.append(line + '\\n')\n",
        "    text = \"\".join(line_set)\n",
        "text = text.lower()  \n",
        "\n",
        "# Tokenizing sentences from processed text\n",
        "vocabulary = {}#initilaizing vocab dict\n",
        "corpus = []#intializing corpus\n",
        "sentence_set = sent_tokenize(text)\n",
        "\n",
        "for sentence in sentence_set: #taking one sentence from sentence list\n",
        "    words = word_tokenize(sentence) #work tokenizing each word of that sentence\n",
        "    processed_sentence = ['<s>'] #making processed sentence out of it and adding START <s> symbol \n",
        "    for word in words:\n",
        "        if len(set(word).intersection(punctuation)) == 0 and (len(word)>1 or word == 'a' or word == 'i') :\n",
        "            processed_sentence.append(word)#adding processed and selected word to my new processed sentence\n",
        "            if word in vocabulary.keys():#adding in vobaulary list if not exist else increasing frequecny by 1 if exists\n",
        "                vocabulary[word] += 1\n",
        "            else:\n",
        "                vocabulary[word] = 1\n",
        "                \n",
        "    processed_sentence.append('</s>')#adding END </s> to the end of my sentence\n",
        "    corpus.append(\" \".join(processed_sentence))#adding processed sentence to the corpus\n",
        "\n",
        "#Now Processed Corpus is ready    \n",
        "number_of_sentence = len(corpus)\n",
        "# Vocabulary list doesn't contain the START and END word used in Pre-Processing\n",
        "vocabulary['<s>'] = 2*number_of_sentence\n",
        "vocabulary['</s>'] = 2*number_of_sentence\n",
        "# Now we have Processed sentence corpus and vocabulary list. \n",
        "#Processed Corpus DATA ANALYSIS  \n",
        "tokens = sum(vocabulary.values())\n",
        "types = len(vocabulary)\n",
        "print ('Number of Sentences = ' + str(number_of_sentence))\n",
        "print ('Number of Tokens = ' + str(tokens))\n",
        "print ('Number of Types = ' + str(types))\n",
        "print (\"TTR= \" + str(types/tokens))\n",
        "# train-test split in 80:20 ratio\n",
        "train_data, test_data = train_test_split(corpus, test_size=0.20, random_state=42)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Sentences = 35056\n",
            "Number of Tokens = 914604\n",
            "Number of Types = 14651\n",
            "TTR= 0.01601895465141198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg-sAza9MwmZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d991e48-9c05-44ba-fb8b-0fa05498b2c1"
      },
      "source": [
        "#defining function for calculting elapsed Time, read data and bild dataset- MODIFIED THE CODE GIVEN IN CODING SESSION \n",
        "\n",
        "def read_data(content):\n",
        "    content = [x.strip() for x in content]\n",
        "    content = [word for i in range(len(content)) for word in content[i].split()]\n",
        "    content = np.array(content)\n",
        "    return content\n",
        "\n",
        "def elapsed(sec):\n",
        "    if sec<60:\n",
        "        return str(sec) + \" sec\"\n",
        "    elif sec<(60*60):\n",
        "        return str(sec/60) + \" min\"\n",
        "    else:\n",
        "        return str(sec/(60*60)) + \" hr\"\n",
        "\n",
        "def build_dataset(words):\n",
        "    count = collections.Counter(words).most_common()\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return dictionary, reverse_dictionary\n",
        "\n",
        "# Importing Train Data and defining build dataset function - USED CODE GIVEN IN CODING SESSION\n",
        "start_time = time.time()\n",
        "training_data = read_data(train_data)\n",
        "testing_data = read_data(test_data)\n",
        "print(\"Loaded training data...\")\n",
        "# CREATING DICTIONARY USING build_dataset function\n",
        "dictionary, reverse_dictionary = build_dataset(training_data)\n",
        "vocab_size = len(dictionary)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded training data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p28wtguNKnNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining the train_text and decoding function- as done in coding session\n",
        "# A dictionary mapping words to an integer index\n",
        "word_index = dictionary.copy()\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<UNK>\"] = 1  # unknown\n",
        "word_index[\"<UNUSED>\"] = 2\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "train_text = []\n",
        "for i in train_data:\n",
        "  train_text.append(decode_review(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkdQu05-d78c",
        "colab_type": "code",
        "outputId": "b9acc251-1271-4d35-c0ec-c03981888b90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#CODE FOR TASK-2a Vanilla RNN BASED LANGUAGE GENERATION (QUADGRAMS)\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.0001\n",
        "training_iters = 40000\n",
        "display_step = 1000\n",
        "#CHANGE VALUE OF n_gram for getting N-GRAM MODEL\n",
        "n_gram = 4\n",
        "\n",
        "\n",
        "logs_path = '/tmp/tensorflow/rnn_words'\n",
        "writer = tf.summary.FileWriter(logs_path)\n",
        "\n",
        "# number of units in RNN cell\n",
        "n_hidden = 128\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(\"float\", [None, n_gram, 1])\n",
        "y = tf.placeholder(\"float\", [None, vocab_size])\n",
        "\n",
        "# RNN output node weights and biases\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
        "}\n",
        "\n",
        "def RNN(x, weights, biases):\n",
        "\n",
        "    x = tf.reshape(x, [-1, n_gram])\n",
        "    x = tf.split(x,n_gram,1)\n",
        "    rnn_cell = rnn.BasicRNNCell(n_hidden)\n",
        "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "pred = RNN(x, weights, biases)\n",
        "\n",
        "# Loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Model evaluation\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    step = 0\n",
        "    offset = random.randint(0,n_gram+1)\n",
        "    end_offset = n_gram + 1\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "    loss_overall = 0\n",
        "    writer.add_graph(session.graph)\n",
        "\n",
        "    while step < training_iters:\n",
        "        # Generate a minibatch. Add some randomness on selection process.\n",
        "        if offset > (len(training_data)-end_offset):\n",
        "            offset = random.randint(0, n_gram+1)\n",
        "\n",
        "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_gram) ]\n",
        "        symbols_in_keys = list(np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1]))\n",
        "\n",
        "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
        "        symbols_out_onehot[dictionary[str(training_data[offset+n_gram])]] = 1.0\n",
        "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
        "\n",
        "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
        "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
        "        loss_total += loss\n",
        "        loss_overall += loss\n",
        "        acc_total += acc\n",
        "        if (step+1) % display_step == 0:\n",
        "            print(\"Iteration= \" + str(step+1) + \", Average Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
        "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
        "            acc_total = 0\n",
        "            loss_total = 0\n",
        "            symbols_in = [training_data[i] for i in range(offset, offset + n_gram)]\n",
        "            symbols_out = training_data[offset + n_gram]\n",
        "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
        "        step += 1\n",
        "        offset += (n_gram+1)\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
        "    \n",
        "    num_start = 0\n",
        "    words = [word_index['<PAD>']] * (n_gram-1) + ['<s>']\n",
        "    overall = list(words)\n",
        "    sentence = ''\n",
        "    num_words = 0\n",
        "    while num_start < n_gram and num_words < 500:\n",
        "        try:\n",
        "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1])\n",
        "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
        "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
        "            if onehot_pred_index == dictionary['</s>']:\n",
        "                sentence += \". \"\n",
        "                num_start += 1\n",
        "            else:\n",
        "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        except:\n",
        "            onehot_pred_index = dictionary['the']\n",
        "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        num_words += 1\n",
        "        \n",
        "\n",
        "    print(sentence)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-12-62a8cded1353>:33: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-12-62a8cded1353>:34: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfc64be80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-12-62a8cded1353>:40: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Iteration= 1000, Average Loss= 31.952727, Average Accuracy= 1.20%\n",
            "Iteration= 2000, Average Loss= 24.466701, Average Accuracy= 2.10%\n",
            "Iteration= 3000, Average Loss= 19.005330, Average Accuracy= 3.10%\n",
            "Iteration= 4000, Average Loss= 15.089254, Average Accuracy= 6.50%\n",
            "Iteration= 5000, Average Loss= 12.096991, Average Accuracy= 7.20%\n",
            "Iteration= 6000, Average Loss= 10.254923, Average Accuracy= 6.10%\n",
            "Iteration= 7000, Average Loss= 8.808358, Average Accuracy= 8.00%\n",
            "Iteration= 8000, Average Loss= 8.613359, Average Accuracy= 7.80%\n",
            "Iteration= 9000, Average Loss= 8.056952, Average Accuracy= 6.90%\n",
            "Iteration= 10000, Average Loss= 8.232272, Average Accuracy= 7.10%\n",
            "Iteration= 11000, Average Loss= 8.024562, Average Accuracy= 6.20%\n",
            "Iteration= 12000, Average Loss= 7.829069, Average Accuracy= 8.10%\n",
            "Iteration= 13000, Average Loss= 7.876012, Average Accuracy= 7.60%\n",
            "Iteration= 14000, Average Loss= 7.560537, Average Accuracy= 8.40%\n",
            "Iteration= 15000, Average Loss= 7.831018, Average Accuracy= 8.20%\n",
            "Iteration= 16000, Average Loss= 7.875059, Average Accuracy= 7.10%\n",
            "Iteration= 17000, Average Loss= 7.943267, Average Accuracy= 7.90%\n",
            "Iteration= 18000, Average Loss= 8.082645, Average Accuracy= 7.10%\n",
            "Iteration= 19000, Average Loss= 8.014244, Average Accuracy= 7.60%\n",
            "Iteration= 20000, Average Loss= 7.743335, Average Accuracy= 7.50%\n",
            "Iteration= 21000, Average Loss= 7.897689, Average Accuracy= 6.10%\n",
            "Iteration= 22000, Average Loss= 7.960024, Average Accuracy= 8.00%\n",
            "Iteration= 23000, Average Loss= 7.835185, Average Accuracy= 7.80%\n",
            "Iteration= 24000, Average Loss= 7.767989, Average Accuracy= 7.50%\n",
            "Iteration= 25000, Average Loss= 7.785463, Average Accuracy= 8.10%\n",
            "Iteration= 26000, Average Loss= 7.712575, Average Accuracy= 8.00%\n",
            "Iteration= 27000, Average Loss= 7.951982, Average Accuracy= 6.80%\n",
            "Iteration= 28000, Average Loss= 7.865804, Average Accuracy= 7.50%\n",
            "Iteration= 29000, Average Loss= 8.038438, Average Accuracy= 7.40%\n",
            "Iteration= 30000, Average Loss= 7.792460, Average Accuracy= 7.90%\n",
            "Iteration= 31000, Average Loss= 7.732976, Average Accuracy= 7.50%\n",
            "Iteration= 32000, Average Loss= 8.004907, Average Accuracy= 6.40%\n",
            "Iteration= 33000, Average Loss= 7.938312, Average Accuracy= 7.30%\n",
            "Iteration= 34000, Average Loss= 7.972815, Average Accuracy= 7.40%\n",
            "Iteration= 35000, Average Loss= 7.920700, Average Accuracy= 8.60%\n",
            "Iteration= 36000, Average Loss= 7.655969, Average Accuracy= 6.70%\n",
            "Iteration= 37000, Average Loss= 7.991604, Average Accuracy= 7.50%\n",
            "Iteration= 38000, Average Loss= 7.957010, Average Accuracy= 6.50%\n",
            "Iteration= 39000, Average Loss= 7.915235, Average Accuracy= 8.30%\n",
            "Iteration= 40000, Average Loss= 8.044976, Average Accuracy= 6.30%\n",
            "Optimization Finished!\n",
            "Elapsed time:  4.749146739641826 min\n",
            " the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnToOoNPxvGE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f23b39f-3b57-4fcd-cbdb-a656a4c33c00"
      },
      "source": [
        "#TASK_2 FINDING PERPLEXITY \n",
        "perplexity = tf.exp(loss)\n",
        "with tf.Session() as session:\n",
        "  print(\"Perplexity for RNN Based Quadram MOdel= \"+str(session.run(perplexity)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity for RNN Based Quadram MOdel= 64720.492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTsd0-JYVvr6",
        "colab_type": "code",
        "outputId": "65940eaa-aea1-4e22-84f4-ec9ea18d0998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        }
      },
      "source": [
        "# SAME CODE JUST CHANGED THE MODEL TO LSTM BASED\n",
        "tf.reset_default_graph()\n",
        "# tf Graph input\n",
        "x = tf.placeholder(\"float\", [None, n_gram, 1])\n",
        "y = tf.placeholder(\"float\", [None, vocab_size])\n",
        "\n",
        "# RNN output node weights and biases\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
        "}\n",
        "\n",
        "def RNN(x, weights, biases):\n",
        "\n",
        "    x = tf.reshape(x, [-1, n_gram])\n",
        "\n",
        "    # Generate a n_input-element sequence of inputs\n",
        "    x = tf.split(x,n_gram,1)\n",
        "\n",
        "    # 1-layer LSTM with n_hidden units \n",
        "    rnn_cell = rnn.LSTMCell(n_hidden, reuse =tf.AUTO_REUSE)\n",
        "\n",
        "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "    # there are n_input outputs but we only want the last output\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "pred = RNN(x, weights, biases)\n",
        "\n",
        "# Loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Model evaluation\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    step = 0\n",
        "    offset = random.randint(0,n_gram+1)\n",
        "    end_offset = n_gram + 1\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "    loss_overall = 0\n",
        "    writer.add_graph(session.graph)\n",
        "\n",
        "    while step < training_iters:\n",
        "        # Generate a minibatch. Add some randomness on selection process.\n",
        "        if offset > (len(training_data)-end_offset):\n",
        "            offset = random.randint(0, n_gram+1)\n",
        "\n",
        "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_gram) ]\n",
        "        symbols_in_keys = list(np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1]))\n",
        "\n",
        "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
        "        symbols_out_onehot[dictionary[str(training_data[offset+n_gram])]] = 1.0\n",
        "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
        "\n",
        "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
        "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
        "        loss_total += loss\n",
        "        loss_overall += loss\n",
        "        acc_total += acc\n",
        "        if (step+1) % display_step == 0:\n",
        "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
        "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
        "            acc_total = 0\n",
        "            loss_total = 0\n",
        "            symbols_in = [training_data[i] for i in range(offset, offset + n_gram)]\n",
        "            symbols_out = training_data[offset + n_gram]\n",
        "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
        "            \n",
        "        step += 1\n",
        "        offset += (n_gram+1)\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
        "    \n",
        "    num_start = 0\n",
        "    words = [word_index['<PAD>']] * (n_gram-1) + ['<s>']\n",
        "    overall = list(words)\n",
        "    sentence = ''\n",
        "    num_words = 0\n",
        "    while num_start < 5 and num_words < 500:\n",
        "        try:\n",
        "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1])\n",
        "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
        "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
        "            if onehot_pred_index == dictionary['</s>']:\n",
        "                sentence += \". \"\n",
        "                num_start += 1\n",
        "            else:\n",
        "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        except:\n",
        "            onehot_pred_index = dictionary['the']\n",
        "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        num_words += 1\n",
        "        \n",
        "\n",
        "    print(sentence)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-14-69864b980d6b>:22: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df056bf98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "Iter= 1000, Average Loss= 13.878376, Average Accuracy= 0.00%\n",
            "Iter= 2000, Average Loss= 10.719683, Average Accuracy= 0.40%\n",
            "Iter= 3000, Average Loss= 9.785787, Average Accuracy= 3.50%\n",
            "Iter= 4000, Average Loss= 9.282362, Average Accuracy= 5.50%\n",
            "Iter= 5000, Average Loss= 8.891389, Average Accuracy= 6.00%\n",
            "Iter= 6000, Average Loss= 8.429275, Average Accuracy= 7.60%\n",
            "Iter= 7000, Average Loss= 8.226546, Average Accuracy= 7.90%\n",
            "Iter= 8000, Average Loss= 8.079347, Average Accuracy= 8.50%\n",
            "Iter= 9000, Average Loss= 7.642736, Average Accuracy= 7.70%\n",
            "Iter= 10000, Average Loss= 7.690950, Average Accuracy= 7.40%\n",
            "Iter= 11000, Average Loss= 7.433869, Average Accuracy= 8.30%\n",
            "Iter= 12000, Average Loss= 7.619117, Average Accuracy= 7.50%\n",
            "Iter= 13000, Average Loss= 7.392291, Average Accuracy= 7.40%\n",
            "Iter= 14000, Average Loss= 7.406154, Average Accuracy= 7.60%\n",
            "Iter= 15000, Average Loss= 7.319507, Average Accuracy= 7.90%\n",
            "Iter= 16000, Average Loss= 7.389556, Average Accuracy= 8.90%\n",
            "Iter= 17000, Average Loss= 7.406141, Average Accuracy= 7.40%\n",
            "Iter= 18000, Average Loss= 7.059468, Average Accuracy= 10.40%\n",
            "Iter= 19000, Average Loss= 7.072238, Average Accuracy= 8.70%\n",
            "Iter= 20000, Average Loss= 7.241516, Average Accuracy= 8.50%\n",
            "Iter= 21000, Average Loss= 7.091917, Average Accuracy= 8.70%\n",
            "Iter= 22000, Average Loss= 7.177413, Average Accuracy= 8.90%\n",
            "Iter= 23000, Average Loss= 7.301244, Average Accuracy= 7.30%\n",
            "Iter= 24000, Average Loss= 7.247603, Average Accuracy= 7.80%\n",
            "Iter= 25000, Average Loss= 6.943491, Average Accuracy= 8.70%\n",
            "Iter= 26000, Average Loss= 7.154730, Average Accuracy= 9.30%\n",
            "Iter= 27000, Average Loss= 7.067544, Average Accuracy= 9.40%\n",
            "Iter= 28000, Average Loss= 7.012418, Average Accuracy= 8.40%\n",
            "Iter= 29000, Average Loss= 7.059521, Average Accuracy= 9.00%\n",
            "Iter= 30000, Average Loss= 7.042675, Average Accuracy= 8.00%\n",
            "Iter= 31000, Average Loss= 6.909835, Average Accuracy= 9.00%\n",
            "Iter= 32000, Average Loss= 7.100993, Average Accuracy= 8.20%\n",
            "Iter= 33000, Average Loss= 6.953212, Average Accuracy= 9.10%\n",
            "Iter= 34000, Average Loss= 6.972299, Average Accuracy= 10.10%\n",
            "Iter= 35000, Average Loss= 7.071833, Average Accuracy= 7.80%\n",
            "Iter= 36000, Average Loss= 7.170125, Average Accuracy= 7.60%\n",
            "Iter= 37000, Average Loss= 6.898300, Average Accuracy= 9.70%\n",
            "Iter= 38000, Average Loss= 7.061841, Average Accuracy= 8.20%\n",
            "Iter= 39000, Average Loss= 7.185522, Average Accuracy= 8.40%\n",
            "Iter= 40000, Average Loss= 7.007940, Average Accuracy= 10.10%\n",
            "Optimization Finished!\n",
            "Elapsed time:  11.247406582037607 min\n",
            " the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_dZ0I9jXJUP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51406e46-28e2-4090-dcdb-67b235b12f4e"
      },
      "source": [
        "\n",
        "perplexity_LSTM =tf.exp(loss)\n",
        "with tf.Session() as session:\n",
        "  print(\"Perplexity for QUADRAM LSTM BASED model= \"+ str(session.run(perplexity_LSTM)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity for QUADRAM LSTM BASED model= 1795.1171\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW3t8VE7auFn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b8bd3ed-8503-43ce-d213-c118b4eeba3a"
      },
      "source": [
        "#CODE FOR TASK-2a Vanilla RNN BASED LANGUAGE GENERATION (TRIGRAMS)\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.0001\n",
        "training_iters = 40000\n",
        "display_step = 1000\n",
        "#CHANGE VALUE OF n_gram for getting N-GRAM MODEL\n",
        "n_gram = 3\n",
        "\n",
        "\n",
        "logs_path = '/tmp/tensorflow/rnn_words'\n",
        "writer = tf.summary.FileWriter(logs_path)\n",
        "\n",
        "# number of units in RNN cell\n",
        "n_hidden = 128\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(\"float\", [None, n_gram, 1])\n",
        "y = tf.placeholder(\"float\", [None, vocab_size])\n",
        "\n",
        "# RNN output node weights and biases\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
        "}\n",
        "\n",
        "def RNN(x, weights, biases):\n",
        "\n",
        "    x = tf.reshape(x, [-1, n_gram])\n",
        "    x = tf.split(x,n_gram,1)\n",
        "    rnn_cell = rnn.BasicRNNCell(n_hidden)\n",
        "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "pred = RNN(x, weights, biases)\n",
        "\n",
        "# Loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Model evaluation\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    step = 0\n",
        "    offset = random.randint(0,n_gram+1)\n",
        "    end_offset = n_gram + 1\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "    loss_overall = 0\n",
        "    writer.add_graph(session.graph)\n",
        "\n",
        "    while step < training_iters:\n",
        "        # Generate a minibatch. Add some randomness on selection process.\n",
        "        if offset > (len(training_data)-end_offset):\n",
        "            offset = random.randint(0, n_gram+1)\n",
        "\n",
        "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_gram) ]\n",
        "        symbols_in_keys = list(np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1]))\n",
        "\n",
        "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
        "        symbols_out_onehot[dictionary[str(training_data[offset+n_gram])]] = 1.0\n",
        "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
        "\n",
        "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
        "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
        "        loss_total += loss\n",
        "        loss_overall += loss\n",
        "        acc_total += acc\n",
        "        if (step+1) % display_step == 0:\n",
        "            print(\"Iteration= \" + str(step+1) + \", Average Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
        "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
        "            acc_total = 0\n",
        "            loss_total = 0\n",
        "            symbols_in = [training_data[i] for i in range(offset, offset + n_gram)]\n",
        "            symbols_out = training_data[offset + n_gram]\n",
        "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
        "        step += 1\n",
        "        offset += (n_gram+1)\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
        "    \n",
        "    num_start = 0\n",
        "    words = [word_index['<PAD>']] * (n_gram-1) + ['<s>']\n",
        "    overall = list(words)\n",
        "    sentence = ''\n",
        "    num_words = 0\n",
        "    while num_start < n_gram and num_words < 500:\n",
        "        try:\n",
        "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1])\n",
        "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
        "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
        "            if onehot_pred_index == dictionary['</s>']:\n",
        "                sentence += \". \"\n",
        "                num_start += 1\n",
        "            else:\n",
        "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        except:\n",
        "            onehot_pred_index = dictionary['the']\n",
        "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        num_words += 1\n",
        "        \n",
        "\n",
        "    print(sentence)\n",
        "    \n",
        "\n",
        "#TASK_2 FINDING PERPLEXITY \n",
        "perplexity = tf.exp(loss)\n",
        "with tf.Session() as session:\n",
        "  print(\"Perplexity for RNN Trigram MOdel= \"+ str(session.run(perplexity)))\n",
        "\n",
        "  \n",
        "# SAME CODE JUST CHANGED THE MODEL TO LSTM BASED\n",
        "tf.reset_default_graph()\n",
        "# tf Graph input\n",
        "x = tf.placeholder(\"float\", [None, n_gram, 1])\n",
        "y = tf.placeholder(\"float\", [None, vocab_size])\n",
        "\n",
        "# RNN output node weights and biases\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
        "}\n",
        "\n",
        "def RNN(x, weights, biases):\n",
        "\n",
        "    x = tf.reshape(x, [-1, n_gram])\n",
        "\n",
        "    # Generate a n_input-element sequence of inputs\n",
        "    x = tf.split(x,n_gram,1)\n",
        "\n",
        "    # 1-layer LSTM with n_hidden units \n",
        "    rnn_cell = rnn.LSTMCell(n_hidden, reuse =tf.AUTO_REUSE)\n",
        "\n",
        "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "    # there are n_input outputs but we only want the last output\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "pred = RNN(x, weights, biases)\n",
        "\n",
        "# Loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Model evaluation\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    step = 0\n",
        "    offset = random.randint(0,n_gram+1)\n",
        "    end_offset = n_gram + 1\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "    loss_overall = 0\n",
        "    writer.add_graph(session.graph)\n",
        "\n",
        "    while step < training_iters:\n",
        "        # Generate a minibatch. Add some randomness on selection process.\n",
        "        if offset > (len(training_data)-end_offset):\n",
        "            offset = random.randint(0, n_gram+1)\n",
        "\n",
        "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_gram) ]\n",
        "        symbols_in_keys = list(np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1]))\n",
        "\n",
        "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
        "        symbols_out_onehot[dictionary[str(training_data[offset+n_gram])]] = 1.0\n",
        "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
        "\n",
        "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
        "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
        "        loss_total += loss\n",
        "        loss_overall += loss\n",
        "        acc_total += acc\n",
        "        if (step+1) % display_step == 0:\n",
        "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
        "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
        "            acc_total = 0\n",
        "            loss_total = 0\n",
        "            symbols_in = [training_data[i] for i in range(offset, offset + n_gram)]\n",
        "            symbols_out = training_data[offset + n_gram]\n",
        "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
        "            \n",
        "        step += 1\n",
        "        offset += (n_gram+1)\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
        "    \n",
        "    num_start = 0\n",
        "    words = [word_index['<PAD>']] * (n_gram-1) + ['<s>']\n",
        "    overall = list(words)\n",
        "    sentence = ''\n",
        "    num_words = 0\n",
        "    while num_start < 5 and num_words < 500:\n",
        "        try:\n",
        "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1])\n",
        "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
        "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
        "            if onehot_pred_index == dictionary['</s>']:\n",
        "                sentence += \". \"\n",
        "                num_start += 1\n",
        "            else:\n",
        "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        except:\n",
        "            onehot_pred_index = dictionary['the']\n",
        "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        num_words += 1\n",
        "        \n",
        "\n",
        "    print(sentence)\n",
        " \n",
        "\n",
        "#TASK_2 FINDING PERPLEXITY \n",
        "perplexity = tf.exp(loss)\n",
        "with tf.Session() as session:\n",
        "  print(\"Perplexity for LSTM Trigram MOdel= \"+ str(session.run(perplexity)))\n",
        "\n",
        "    "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bd68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bd68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bd68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bd68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bd68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bd68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "Iteration= 1000, Average Loss= 29.739771, Average Accuracy= 0.70%\n",
            "Iteration= 2000, Average Loss= 22.695530, Average Accuracy= 2.10%\n",
            "Iteration= 3000, Average Loss= 17.704835, Average Accuracy= 1.90%\n",
            "Iteration= 4000, Average Loss= 14.154064, Average Accuracy= 3.50%\n",
            "Iteration= 5000, Average Loss= 11.645088, Average Accuracy= 5.90%\n",
            "Iteration= 6000, Average Loss= 10.247011, Average Accuracy= 5.50%\n",
            "Iteration= 7000, Average Loss= 9.084210, Average Accuracy= 8.90%\n",
            "Iteration= 8000, Average Loss= 8.487877, Average Accuracy= 7.30%\n",
            "Iteration= 9000, Average Loss= 8.191763, Average Accuracy= 8.10%\n",
            "Iteration= 10000, Average Loss= 8.260988, Average Accuracy= 7.90%\n",
            "Iteration= 11000, Average Loss= 8.032297, Average Accuracy= 8.70%\n",
            "Iteration= 12000, Average Loss= 8.227926, Average Accuracy= 7.00%\n",
            "Iteration= 13000, Average Loss= 8.048016, Average Accuracy= 7.60%\n",
            "Iteration= 14000, Average Loss= 7.970814, Average Accuracy= 7.50%\n",
            "Iteration= 15000, Average Loss= 8.130755, Average Accuracy= 7.40%\n",
            "Iteration= 16000, Average Loss= 8.021326, Average Accuracy= 7.20%\n",
            "Iteration= 17000, Average Loss= 7.974693, Average Accuracy= 8.20%\n",
            "Iteration= 18000, Average Loss= 7.910351, Average Accuracy= 8.40%\n",
            "Iteration= 19000, Average Loss= 8.101726, Average Accuracy= 7.60%\n",
            "Iteration= 20000, Average Loss= 7.922966, Average Accuracy= 7.30%\n",
            "Iteration= 21000, Average Loss= 8.218437, Average Accuracy= 8.10%\n",
            "Iteration= 22000, Average Loss= 8.030588, Average Accuracy= 6.80%\n",
            "Iteration= 23000, Average Loss= 7.823764, Average Accuracy= 9.00%\n",
            "Iteration= 24000, Average Loss= 7.955304, Average Accuracy= 7.70%\n",
            "Iteration= 25000, Average Loss= 8.054950, Average Accuracy= 7.50%\n",
            "Iteration= 26000, Average Loss= 8.076346, Average Accuracy= 8.90%\n",
            "Iteration= 27000, Average Loss= 8.065031, Average Accuracy= 7.00%\n",
            "Iteration= 28000, Average Loss= 7.967192, Average Accuracy= 6.40%\n",
            "Iteration= 29000, Average Loss= 7.961704, Average Accuracy= 7.20%\n",
            "Iteration= 30000, Average Loss= 7.987449, Average Accuracy= 7.10%\n",
            "Iteration= 31000, Average Loss= 7.960457, Average Accuracy= 6.80%\n",
            "Iteration= 32000, Average Loss= 7.762759, Average Accuracy= 7.40%\n",
            "Iteration= 33000, Average Loss= 7.720324, Average Accuracy= 9.00%\n",
            "Iteration= 34000, Average Loss= 7.981161, Average Accuracy= 8.00%\n",
            "Iteration= 35000, Average Loss= 8.151045, Average Accuracy= 6.60%\n",
            "Iteration= 36000, Average Loss= 7.857469, Average Accuracy= 7.60%\n",
            "Iteration= 37000, Average Loss= 7.709912, Average Accuracy= 6.80%\n",
            "Iteration= 38000, Average Loss= 7.828970, Average Accuracy= 7.70%\n",
            "Iteration= 39000, Average Loss= 8.005667, Average Accuracy= 5.60%\n",
            "Iteration= 40000, Average Loss= 7.895968, Average Accuracy= 9.10%\n",
            "Optimization Finished!\n",
            "Elapsed time:  15.518195919195811 min\n",
            " the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "Perplexity for RNN Trigram MOdel= 47.634594\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0dae6bbe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0dae6bbe80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0dae6bbe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0dae6bbe80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0dae6bbe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0dae6bbe80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0dae6bbe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0dae6bbe80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0dae6bbe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0dae6bbe80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0dae6bbe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0dae6bbe80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "Iter= 1000, Average Loss= 14.612545, Average Accuracy= 0.10%\n",
            "Iter= 2000, Average Loss= 11.738199, Average Accuracy= 0.70%\n",
            "Iter= 3000, Average Loss= 10.311771, Average Accuracy= 3.30%\n",
            "Iter= 4000, Average Loss= 9.475206, Average Accuracy= 7.40%\n",
            "Iter= 5000, Average Loss= 9.063316, Average Accuracy= 6.40%\n",
            "Iter= 6000, Average Loss= 8.680754, Average Accuracy= 6.80%\n",
            "Iter= 7000, Average Loss= 8.304137, Average Accuracy= 7.00%\n",
            "Iter= 8000, Average Loss= 8.149859, Average Accuracy= 7.40%\n",
            "Iter= 9000, Average Loss= 7.747834, Average Accuracy= 10.10%\n",
            "Iter= 10000, Average Loss= 7.701513, Average Accuracy= 9.30%\n",
            "Iter= 11000, Average Loss= 7.719847, Average Accuracy= 6.40%\n",
            "Iter= 12000, Average Loss= 7.601649, Average Accuracy= 7.90%\n",
            "Iter= 13000, Average Loss= 7.560138, Average Accuracy= 8.50%\n",
            "Iter= 14000, Average Loss= 7.537420, Average Accuracy= 7.90%\n",
            "Iter= 15000, Average Loss= 7.385321, Average Accuracy= 8.50%\n",
            "Iter= 16000, Average Loss= 7.511715, Average Accuracy= 5.90%\n",
            "Iter= 17000, Average Loss= 7.361090, Average Accuracy= 8.60%\n",
            "Iter= 18000, Average Loss= 7.265268, Average Accuracy= 8.40%\n",
            "Iter= 19000, Average Loss= 7.100511, Average Accuracy= 8.40%\n",
            "Iter= 20000, Average Loss= 7.389345, Average Accuracy= 6.80%\n",
            "Iter= 21000, Average Loss= 7.178508, Average Accuracy= 8.00%\n",
            "Iter= 22000, Average Loss= 7.240966, Average Accuracy= 9.40%\n",
            "Iter= 23000, Average Loss= 7.166218, Average Accuracy= 9.40%\n",
            "Iter= 24000, Average Loss= 7.147657, Average Accuracy= 7.20%\n",
            "Iter= 25000, Average Loss= 7.140289, Average Accuracy= 8.40%\n",
            "Iter= 26000, Average Loss= 7.329789, Average Accuracy= 8.10%\n",
            "Iter= 27000, Average Loss= 7.209694, Average Accuracy= 9.10%\n",
            "Iter= 28000, Average Loss= 7.038633, Average Accuracy= 9.50%\n",
            "Iter= 29000, Average Loss= 7.029883, Average Accuracy= 7.40%\n",
            "Iter= 30000, Average Loss= 7.177272, Average Accuracy= 8.70%\n",
            "Iter= 31000, Average Loss= 7.104064, Average Accuracy= 7.80%\n",
            "Iter= 32000, Average Loss= 7.034372, Average Accuracy= 9.60%\n",
            "Iter= 33000, Average Loss= 7.061329, Average Accuracy= 8.50%\n",
            "Iter= 34000, Average Loss= 6.970862, Average Accuracy= 10.70%\n",
            "Iter= 35000, Average Loss= 6.954546, Average Accuracy= 9.60%\n",
            "Iter= 36000, Average Loss= 6.943954, Average Accuracy= 8.90%\n",
            "Iter= 37000, Average Loss= 7.016916, Average Accuracy= 7.30%\n",
            "Iter= 38000, Average Loss= 6.960778, Average Accuracy= 9.30%\n",
            "Iter= 39000, Average Loss= 7.106553, Average Accuracy= 10.60%\n",
            "Iter= 40000, Average Loss= 7.130868, Average Accuracy= 8.40%\n",
            "Optimization Finished!\n",
            "Elapsed time:  20.169282750288644 min\n",
            " the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "Perplexity for LSTM Trigram MOdel= 26266.615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pipARUU6zBy4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f269feec-0518-405e-ab32-58e6027d07c0"
      },
      "source": [
        "#CODE FOR TASK-2a Vanilla RNN BASED LANGUAGE GENERATION (BIGRAMS)\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.0001\n",
        "training_iters = 40000\n",
        "display_step = 1000\n",
        "#CHANGE VALUE OF n_gram for getting N-GRAM MODEL\n",
        "n_gram = 2\n",
        "\n",
        "\n",
        "logs_path = '/tmp/tensorflow/rnn_words'\n",
        "writer = tf.summary.FileWriter(logs_path)\n",
        "\n",
        "# number of units in RNN cell\n",
        "n_hidden = 128\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(\"float\", [None, n_gram, 1])\n",
        "y = tf.placeholder(\"float\", [None, vocab_size])\n",
        "\n",
        "# RNN output node weights and biases\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
        "}\n",
        "\n",
        "def RNN(x, weights, biases):\n",
        "\n",
        "    x = tf.reshape(x, [-1, n_gram])\n",
        "    x = tf.split(x,n_gram,1)\n",
        "    rnn_cell = rnn.BasicRNNCell(n_hidden)\n",
        "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "pred = RNN(x, weights, biases)\n",
        "\n",
        "# Loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Model evaluation\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    step = 0\n",
        "    offset = random.randint(0,n_gram+1)\n",
        "    end_offset = n_gram + 1\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "    loss_overall = 0\n",
        "    writer.add_graph(session.graph)\n",
        "\n",
        "    while step < training_iters:\n",
        "        # Generate a minibatch. Add some randomness on selection process.\n",
        "        if offset > (len(training_data)-end_offset):\n",
        "            offset = random.randint(0, n_gram+1)\n",
        "\n",
        "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_gram) ]\n",
        "        symbols_in_keys = list(np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1]))\n",
        "\n",
        "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
        "        symbols_out_onehot[dictionary[str(training_data[offset+n_gram])]] = 1.0\n",
        "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
        "\n",
        "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
        "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
        "        loss_total += loss\n",
        "        loss_overall += loss\n",
        "        acc_total += acc\n",
        "        if (step+1) % display_step == 0:\n",
        "            print(\"Iteration= \" + str(step+1) + \", Average Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
        "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
        "            acc_total = 0\n",
        "            loss_total = 0\n",
        "            symbols_in = [training_data[i] for i in range(offset, offset + n_gram)]\n",
        "            symbols_out = training_data[offset + n_gram]\n",
        "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
        "        step += 1\n",
        "        offset += (n_gram+1)\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
        "    \n",
        "    num_start = 0\n",
        "    words = [word_index['<PAD>']] * (n_gram-1) + ['<s>']\n",
        "    overall = list(words)\n",
        "    sentence = ''\n",
        "    num_words = 0\n",
        "    while num_start < n_gram and num_words < 500:\n",
        "        try:\n",
        "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1])\n",
        "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
        "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
        "            if onehot_pred_index == dictionary['</s>']:\n",
        "                sentence += \". \"\n",
        "                num_start += 1\n",
        "            else:\n",
        "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        except:\n",
        "            onehot_pred_index = dictionary['the']\n",
        "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        num_words += 1\n",
        "        \n",
        "\n",
        "    print(sentence)\n",
        "    \n",
        "\n",
        "#TASK_2 FINDING PERPLEXITY \n",
        "perplexity = tf.exp(loss)\n",
        "with tf.Session() as session:\n",
        "  print(\"Perplexity for RNN Bigram MOdel= \"+ str(session.run(perplexity)))\n",
        "\n",
        "  \n",
        "# SAME CODE JUST CHANGED THE MODEL TO LSTM BASED\n",
        "tf.reset_default_graph()\n",
        "# tf Graph input\n",
        "x = tf.placeholder(\"float\", [None, n_gram, 1])\n",
        "y = tf.placeholder(\"float\", [None, vocab_size])\n",
        "\n",
        "# RNN output node weights and biases\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
        "}\n",
        "\n",
        "def RNN(x, weights, biases):\n",
        "\n",
        "    x = tf.reshape(x, [-1, n_gram])\n",
        "\n",
        "    # Generate a n_input-element sequence of inputs\n",
        "    x = tf.split(x,n_gram,1)\n",
        "\n",
        "    # 1-layer LSTM with n_hidden units \n",
        "    rnn_cell = rnn.LSTMCell(n_hidden, reuse =tf.AUTO_REUSE)\n",
        "\n",
        "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "    # there are n_input outputs but we only want the last output\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "pred = RNN(x, weights, biases)\n",
        "\n",
        "# Loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Model evaluation\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    step = 0\n",
        "    offset = random.randint(0,n_gram+1)\n",
        "    end_offset = n_gram + 1\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "    loss_overall = 0\n",
        "    writer.add_graph(session.graph)\n",
        "\n",
        "    while step < training_iters:\n",
        "        # Generate a minibatch. Add some randomness on selection process.\n",
        "        if offset > (len(training_data)-end_offset):\n",
        "            offset = random.randint(0, n_gram+1)\n",
        "\n",
        "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_gram) ]\n",
        "        symbols_in_keys = list(np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1]))\n",
        "\n",
        "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
        "        symbols_out_onehot[dictionary[str(training_data[offset+n_gram])]] = 1.0\n",
        "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
        "\n",
        "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
        "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
        "        loss_total += loss\n",
        "        loss_overall += loss\n",
        "        acc_total += acc\n",
        "        if (step+1) % display_step == 0:\n",
        "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
        "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
        "            acc_total = 0\n",
        "            loss_total = 0\n",
        "            symbols_in = [training_data[i] for i in range(offset, offset + n_gram)]\n",
        "            symbols_out = training_data[offset + n_gram]\n",
        "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
        "            \n",
        "        step += 1\n",
        "        offset += (n_gram+1)\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
        "    \n",
        "    num_start = 0\n",
        "    words = [word_index['<PAD>']] * (n_gram-1) + ['<s>']\n",
        "    overall = list(words)\n",
        "    sentence = ''\n",
        "    num_words = 0\n",
        "    while num_start < 5 and num_words < 500:\n",
        "        try:\n",
        "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1])\n",
        "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
        "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
        "            if onehot_pred_index == dictionary['</s>']:\n",
        "                sentence += \". \"\n",
        "                num_start += 1\n",
        "            else:\n",
        "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        except:\n",
        "            onehot_pred_index = dictionary['the']\n",
        "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        num_words += 1\n",
        "        \n",
        "\n",
        "    print(sentence)\n",
        " \n",
        "\n",
        "#TASK_2 FINDING PERPLEXITY \n",
        "perplexity = tf.exp(loss)\n",
        "with tf.Session() as session:\n",
        "  print(\"Perplexity for LSTM Bigram MOdel= \"+ str(session.run(perplexity)))\n",
        "\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0df004bda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "Iteration= 1000, Average Loss= 29.840953, Average Accuracy= 0.80%\n",
            "Iteration= 2000, Average Loss= 22.384513, Average Accuracy= 2.70%\n",
            "Iteration= 3000, Average Loss= 18.773488, Average Accuracy= 0.50%\n",
            "Iteration= 4000, Average Loss= 14.604409, Average Accuracy= 2.70%\n",
            "Iteration= 5000, Average Loss= 11.732413, Average Accuracy= 5.70%\n",
            "Iteration= 6000, Average Loss= 10.430374, Average Accuracy= 8.60%\n",
            "Iteration= 7000, Average Loss= 9.042752, Average Accuracy= 7.80%\n",
            "Iteration= 8000, Average Loss= 8.614702, Average Accuracy= 6.90%\n",
            "Iteration= 9000, Average Loss= 8.236876, Average Accuracy= 7.60%\n",
            "Iteration= 10000, Average Loss= 8.230106, Average Accuracy= 8.10%\n",
            "Iteration= 11000, Average Loss= 8.014003, Average Accuracy= 8.20%\n",
            "Iteration= 12000, Average Loss= 7.891100, Average Accuracy= 8.40%\n",
            "Iteration= 13000, Average Loss= 8.137641, Average Accuracy= 7.90%\n",
            "Iteration= 14000, Average Loss= 7.885893, Average Accuracy= 7.40%\n",
            "Iteration= 15000, Average Loss= 8.070992, Average Accuracy= 7.00%\n",
            "Iteration= 16000, Average Loss= 8.124117, Average Accuracy= 7.40%\n",
            "Iteration= 17000, Average Loss= 8.113383, Average Accuracy= 7.90%\n",
            "Iteration= 18000, Average Loss= 7.868113, Average Accuracy= 8.40%\n",
            "Iteration= 19000, Average Loss= 7.863334, Average Accuracy= 6.80%\n",
            "Iteration= 20000, Average Loss= 8.039889, Average Accuracy= 7.10%\n",
            "Iteration= 21000, Average Loss= 7.981213, Average Accuracy= 7.10%\n",
            "Iteration= 22000, Average Loss= 7.532997, Average Accuracy= 7.80%\n",
            "Iteration= 23000, Average Loss= 7.872530, Average Accuracy= 8.40%\n",
            "Iteration= 24000, Average Loss= 8.019521, Average Accuracy= 6.70%\n",
            "Iteration= 25000, Average Loss= 7.675691, Average Accuracy= 7.60%\n",
            "Iteration= 26000, Average Loss= 7.727450, Average Accuracy= 7.50%\n",
            "Iteration= 27000, Average Loss= 7.674344, Average Accuracy= 9.20%\n",
            "Iteration= 28000, Average Loss= 7.894013, Average Accuracy= 7.40%\n",
            "Iteration= 29000, Average Loss= 7.831470, Average Accuracy= 6.80%\n",
            "Iteration= 30000, Average Loss= 7.941097, Average Accuracy= 8.60%\n",
            "Iteration= 31000, Average Loss= 7.698028, Average Accuracy= 7.00%\n",
            "Iteration= 32000, Average Loss= 7.941281, Average Accuracy= 7.50%\n",
            "Iteration= 33000, Average Loss= 7.905041, Average Accuracy= 7.50%\n",
            "Iteration= 34000, Average Loss= 7.986884, Average Accuracy= 8.70%\n",
            "Iteration= 35000, Average Loss= 8.002987, Average Accuracy= 6.90%\n",
            "Iteration= 36000, Average Loss= 8.062307, Average Accuracy= 8.30%\n",
            "Iteration= 37000, Average Loss= 7.687379, Average Accuracy= 7.50%\n",
            "Iteration= 38000, Average Loss= 7.830865, Average Accuracy= 8.80%\n",
            "Iteration= 39000, Average Loss= 7.685126, Average Accuracy= 9.00%\n",
            "Iteration= 40000, Average Loss= 7.998011, Average Accuracy= 6.80%\n",
            "Optimization Finished!\n",
            "Elapsed time:  34.735939995447794 min\n",
            " the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "Perplexity for RNN QUADgram MOdel= 10015.902\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df05385f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df05385f8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df05385f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df05385f8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df05385f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df05385f8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df05385f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0df05385f8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "Iter= 1000, Average Loss= 14.104280, Average Accuracy= 0.00%\n",
            "Iter= 2000, Average Loss= 11.128673, Average Accuracy= 0.00%\n",
            "Iter= 3000, Average Loss= 10.060490, Average Accuracy= 0.00%\n",
            "Iter= 4000, Average Loss= 9.544789, Average Accuracy= 4.30%\n",
            "Iter= 5000, Average Loss= 8.913687, Average Accuracy= 7.90%\n",
            "Iter= 6000, Average Loss= 8.648725, Average Accuracy= 7.50%\n",
            "Iter= 7000, Average Loss= 8.124939, Average Accuracy= 8.50%\n",
            "Iter= 8000, Average Loss= 8.077865, Average Accuracy= 6.50%\n",
            "Iter= 9000, Average Loss= 7.685583, Average Accuracy= 9.50%\n",
            "Iter= 10000, Average Loss= 8.015095, Average Accuracy= 7.30%\n",
            "Iter= 11000, Average Loss= 7.586874, Average Accuracy= 8.10%\n",
            "Iter= 12000, Average Loss= 7.538503, Average Accuracy= 9.10%\n",
            "Iter= 13000, Average Loss= 7.610879, Average Accuracy= 7.50%\n",
            "Iter= 14000, Average Loss= 7.247074, Average Accuracy= 9.40%\n",
            "Iter= 15000, Average Loss= 7.461945, Average Accuracy= 7.60%\n",
            "Iter= 16000, Average Loss= 7.490046, Average Accuracy= 7.70%\n",
            "Iter= 17000, Average Loss= 7.554575, Average Accuracy= 7.70%\n",
            "Iter= 18000, Average Loss= 7.410467, Average Accuracy= 8.60%\n",
            "Iter= 19000, Average Loss= 7.280251, Average Accuracy= 8.50%\n",
            "Iter= 20000, Average Loss= 7.234581, Average Accuracy= 7.20%\n",
            "Iter= 21000, Average Loss= 7.290310, Average Accuracy= 7.10%\n",
            "Iter= 22000, Average Loss= 7.244209, Average Accuracy= 6.10%\n",
            "Iter= 23000, Average Loss= 7.233081, Average Accuracy= 7.50%\n",
            "Iter= 24000, Average Loss= 7.037152, Average Accuracy= 9.70%\n",
            "Iter= 25000, Average Loss= 7.255985, Average Accuracy= 8.70%\n",
            "Iter= 26000, Average Loss= 7.402274, Average Accuracy= 7.60%\n",
            "Iter= 27000, Average Loss= 7.209386, Average Accuracy= 10.10%\n",
            "Iter= 28000, Average Loss= 7.140800, Average Accuracy= 8.10%\n",
            "Iter= 29000, Average Loss= 7.055227, Average Accuracy= 7.60%\n",
            "Iter= 30000, Average Loss= 7.047269, Average Accuracy= 8.70%\n",
            "Iter= 31000, Average Loss= 7.237485, Average Accuracy= 8.00%\n",
            "Iter= 32000, Average Loss= 7.007288, Average Accuracy= 11.50%\n",
            "Iter= 33000, Average Loss= 7.005772, Average Accuracy= 8.10%\n",
            "Iter= 34000, Average Loss= 7.050642, Average Accuracy= 8.20%\n",
            "Iter= 35000, Average Loss= 7.232083, Average Accuracy= 8.20%\n",
            "Iter= 36000, Average Loss= 7.036693, Average Accuracy= 9.90%\n",
            "Iter= 37000, Average Loss= 7.066964, Average Accuracy= 7.80%\n",
            "Iter= 38000, Average Loss= 7.112336, Average Accuracy= 8.70%\n",
            "Iter= 39000, Average Loss= 7.112219, Average Accuracy= 9.30%\n",
            "Iter= 40000, Average Loss= 7.184038, Average Accuracy= 7.00%\n",
            "Optimization Finished!\n",
            "Elapsed time:  39.11511653661728 min\n",
            " the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "Perplexity for LSTM QUADgram MOdel= 149.82545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feRwz3SxzMDj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8aec440a-6c2b-4a9d-dea7-d88dd914d147"
      },
      "source": [
        "#CODE FOR TASK-2a Vanilla RNN BASED LANGUAGE GENERATION (UNIGRAMS)\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.0001\n",
        "training_iters = 40000\n",
        "display_step = 1000\n",
        "#CHANGE VALUE OF n_gram for getting N-GRAM MODEL\n",
        "n_gram = 1\n",
        "\n",
        "\n",
        "logs_path = '/tmp/tensorflow/rnn_words'\n",
        "writer = tf.summary.FileWriter(logs_path)\n",
        "\n",
        "# number of units in RNN cell\n",
        "n_hidden = 128\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(\"float\", [None, n_gram, 1])\n",
        "y = tf.placeholder(\"float\", [None, vocab_size])\n",
        "\n",
        "# RNN output node weights and biases\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
        "}\n",
        "\n",
        "def RNN(x, weights, biases):\n",
        "\n",
        "    x = tf.reshape(x, [-1, n_gram])\n",
        "    x = tf.split(x,n_gram,1)\n",
        "    rnn_cell = rnn.BasicRNNCell(n_hidden)\n",
        "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "pred = RNN(x, weights, biases)\n",
        "\n",
        "# Loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Model evaluation\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    step = 0\n",
        "    offset = random.randint(0,n_gram+1)\n",
        "    end_offset = n_gram + 1\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "    loss_overall = 0\n",
        "    writer.add_graph(session.graph)\n",
        "\n",
        "    while step < training_iters:\n",
        "        # Generate a minibatch. Add some randomness on selection process.\n",
        "        if offset > (len(training_data)-end_offset):\n",
        "            offset = random.randint(0, n_gram+1)\n",
        "\n",
        "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_gram) ]\n",
        "        symbols_in_keys = list(np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1]))\n",
        "\n",
        "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
        "        symbols_out_onehot[dictionary[str(training_data[offset+n_gram])]] = 1.0\n",
        "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
        "\n",
        "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
        "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
        "        loss_total += loss\n",
        "        loss_overall += loss\n",
        "        acc_total += acc\n",
        "        if (step+1) % display_step == 0:\n",
        "            print(\"Iteration= \" + str(step+1) + \", Average Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
        "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
        "            acc_total = 0\n",
        "            loss_total = 0\n",
        "            symbols_in = [training_data[i] for i in range(offset, offset + n_gram)]\n",
        "            symbols_out = training_data[offset + n_gram]\n",
        "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
        "        step += 1\n",
        "        offset += (n_gram+1)\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
        "    \n",
        "    num_start = 0\n",
        "    words = [word_index['<PAD>']] * (n_gram-1) + ['<s>']\n",
        "    overall = list(words)\n",
        "    sentence = ''\n",
        "    num_words = 0\n",
        "    while num_start < n_gram and num_words < 500:\n",
        "        try:\n",
        "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1])\n",
        "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
        "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
        "            if onehot_pred_index == dictionary['</s>']:\n",
        "                sentence += \". \"\n",
        "                num_start += 1\n",
        "            else:\n",
        "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        except:\n",
        "            onehot_pred_index = dictionary['the']\n",
        "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        num_words += 1\n",
        "        \n",
        "\n",
        "    print(sentence)\n",
        "    \n",
        "\n",
        "#TASK_2 FINDING PERPLEXITY \n",
        "perplexity = tf.exp(loss)\n",
        "with tf.Session() as session:\n",
        "  print(\"Perplexity for RNN Unigram MOdel= \"+ str(session.run(perplexity)))\n",
        "\n",
        "  \n",
        "# SAME CODE JUST CHANGED THE MODEL TO LSTM BASED\n",
        "tf.reset_default_graph()\n",
        "# tf Graph input\n",
        "x = tf.placeholder(\"float\", [None, n_gram, 1])\n",
        "y = tf.placeholder(\"float\", [None, vocab_size])\n",
        "\n",
        "# RNN output node weights and biases\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
        "}\n",
        "\n",
        "def RNN(x, weights, biases):\n",
        "\n",
        "    x = tf.reshape(x, [-1, n_gram])\n",
        "\n",
        "    # Generate a n_input-element sequence of inputs\n",
        "    x = tf.split(x,n_gram,1)\n",
        "\n",
        "    # 1-layer LSTM with n_hidden units \n",
        "    rnn_cell = rnn.LSTMCell(n_hidden, reuse =tf.AUTO_REUSE)\n",
        "\n",
        "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "    # there are n_input outputs but we only want the last output\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "pred = RNN(x, weights, biases)\n",
        "\n",
        "# Loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Model evaluation\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    step = 0\n",
        "    offset = random.randint(0,n_gram+1)\n",
        "    end_offset = n_gram + 1\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "    loss_overall = 0\n",
        "    writer.add_graph(session.graph)\n",
        "\n",
        "    while step < training_iters:\n",
        "        # Generate a minibatch. Add some randomness on selection process.\n",
        "        if offset > (len(training_data)-end_offset):\n",
        "            offset = random.randint(0, n_gram+1)\n",
        "\n",
        "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_gram) ]\n",
        "        symbols_in_keys = list(np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1]))\n",
        "\n",
        "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
        "        symbols_out_onehot[dictionary[str(training_data[offset+n_gram])]] = 1.0\n",
        "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
        "\n",
        "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
        "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
        "        loss_total += loss\n",
        "        loss_overall += loss\n",
        "        acc_total += acc\n",
        "        if (step+1) % display_step == 0:\n",
        "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
        "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
        "            acc_total = 0\n",
        "            loss_total = 0\n",
        "            symbols_in = [training_data[i] for i in range(offset, offset + n_gram)]\n",
        "            symbols_out = training_data[offset + n_gram]\n",
        "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
        "            \n",
        "        step += 1\n",
        "        offset += (n_gram+1)\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
        "    \n",
        "    num_start = 0\n",
        "    words = [word_index['<PAD>']] * (n_gram-1) + ['<s>']\n",
        "    overall = list(words)\n",
        "    sentence = ''\n",
        "    num_words = 0\n",
        "    while num_start < 5 and num_words < 500:\n",
        "        try:\n",
        "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_gram, 1])\n",
        "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
        "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
        "            if onehot_pred_index == dictionary['</s>']:\n",
        "                sentence += \". \"\n",
        "                num_start += 1\n",
        "            else:\n",
        "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        except:\n",
        "            onehot_pred_index = dictionary['the']\n",
        "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        num_words += 1\n",
        "        \n",
        "\n",
        "    print(sentence)\n",
        " \n",
        "\n",
        "#TASK_2 FINDING PERPLEXITY \n",
        "perplexity = tf.exp(loss)\n",
        "with tf.Session() as session:\n",
        "  print(\"Perplexity for LSTM Unigram MOdel= \"+ str(session.run(perplexity)))\n",
        "\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfb4ba1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfb4ba1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfb4ba1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f0dfb4ba1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "Iteration= 1000, Average Loss= 27.839959, Average Accuracy= 0.00%\n",
            "Iteration= 2000, Average Loss= 20.673776, Average Accuracy= 0.00%\n",
            "Iteration= 3000, Average Loss= 15.664599, Average Accuracy= 0.40%\n",
            "Iteration= 4000, Average Loss= 12.179997, Average Accuracy= 1.60%\n",
            "Iteration= 5000, Average Loss= 9.889768, Average Accuracy= 3.30%\n",
            "Iteration= 6000, Average Loss= 8.713016, Average Accuracy= 6.90%\n",
            "Iteration= 7000, Average Loss= 8.471187, Average Accuracy= 7.60%\n",
            "Iteration= 8000, Average Loss= 8.307948, Average Accuracy= 8.30%\n",
            "Iteration= 9000, Average Loss= 8.324764, Average Accuracy= 8.10%\n",
            "Iteration= 10000, Average Loss= 8.129380, Average Accuracy= 8.30%\n",
            "Iteration= 11000, Average Loss= 7.950933, Average Accuracy= 8.10%\n",
            "Iteration= 12000, Average Loss= 8.023893, Average Accuracy= 7.40%\n",
            "Iteration= 13000, Average Loss= 7.958294, Average Accuracy= 8.10%\n",
            "Iteration= 14000, Average Loss= 8.081017, Average Accuracy= 8.20%\n",
            "Iteration= 15000, Average Loss= 8.218112, Average Accuracy= 6.80%\n",
            "Iteration= 16000, Average Loss= 7.923836, Average Accuracy= 6.60%\n",
            "Iteration= 17000, Average Loss= 7.984957, Average Accuracy= 8.50%\n",
            "Iteration= 18000, Average Loss= 7.933294, Average Accuracy= 5.90%\n",
            "Iteration= 19000, Average Loss= 7.894879, Average Accuracy= 7.60%\n",
            "Iteration= 20000, Average Loss= 7.751925, Average Accuracy= 8.60%\n",
            "Iteration= 21000, Average Loss= 7.747457, Average Accuracy= 8.70%\n",
            "Iteration= 22000, Average Loss= 8.097643, Average Accuracy= 6.90%\n",
            "Iteration= 23000, Average Loss= 7.931798, Average Accuracy= 7.30%\n",
            "Iteration= 24000, Average Loss= 7.932382, Average Accuracy= 7.30%\n",
            "Iteration= 25000, Average Loss= 8.020097, Average Accuracy= 6.50%\n",
            "Iteration= 26000, Average Loss= 8.092091, Average Accuracy= 7.90%\n",
            "Iteration= 27000, Average Loss= 8.007935, Average Accuracy= 7.10%\n",
            "Iteration= 28000, Average Loss= 7.985763, Average Accuracy= 7.90%\n",
            "Iteration= 29000, Average Loss= 8.071622, Average Accuracy= 7.40%\n",
            "Iteration= 30000, Average Loss= 7.978769, Average Accuracy= 7.90%\n",
            "Iteration= 31000, Average Loss= 7.957776, Average Accuracy= 7.00%\n",
            "Iteration= 32000, Average Loss= 7.868912, Average Accuracy= 6.40%\n",
            "Iteration= 33000, Average Loss= 7.948543, Average Accuracy= 7.50%\n",
            "Iteration= 34000, Average Loss= 8.016674, Average Accuracy= 8.10%\n",
            "Iteration= 35000, Average Loss= 7.916270, Average Accuracy= 7.30%\n",
            "Iteration= 36000, Average Loss= 7.909046, Average Accuracy= 8.60%\n",
            "Iteration= 37000, Average Loss= 8.152539, Average Accuracy= 7.90%\n",
            "Iteration= 38000, Average Loss= 7.822899, Average Accuracy= 6.10%\n",
            "Iteration= 39000, Average Loss= 8.238883, Average Accuracy= 5.90%\n",
            "Iteration= 40000, Average Loss= 8.087372, Average Accuracy= 7.20%\n",
            "Optimization Finished!\n",
            "Elapsed time:  52.4139127333959 min\n",
            " <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>\n",
            "Perplexity for RNN Unigram MOdel= 16023.112\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0daca6ceb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0daca6ceb8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0daca6ceb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f0daca6ceb8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "Iter= 1000, Average Loss= 13.478291, Average Accuracy= 0.00%\n",
            "Iter= 2000, Average Loss= 11.584321, Average Accuracy= 0.00%\n",
            "Iter= 3000, Average Loss= 10.482070, Average Accuracy= 0.00%\n",
            "Iter= 4000, Average Loss= 9.874664, Average Accuracy= 0.30%\n",
            "Iter= 5000, Average Loss= 9.244196, Average Accuracy= 1.90%\n",
            "Iter= 6000, Average Loss= 9.027161, Average Accuracy= 2.30%\n",
            "Iter= 7000, Average Loss= 8.656886, Average Accuracy= 5.50%\n",
            "Iter= 8000, Average Loss= 8.579702, Average Accuracy= 8.70%\n",
            "Iter= 9000, Average Loss= 8.364678, Average Accuracy= 8.20%\n",
            "Iter= 10000, Average Loss= 8.180603, Average Accuracy= 8.20%\n",
            "Iter= 11000, Average Loss= 7.955666, Average Accuracy= 8.60%\n",
            "Iter= 12000, Average Loss= 7.924553, Average Accuracy= 6.70%\n",
            "Iter= 13000, Average Loss= 7.836331, Average Accuracy= 8.60%\n",
            "Iter= 14000, Average Loss= 7.752571, Average Accuracy= 7.20%\n",
            "Iter= 15000, Average Loss= 7.752608, Average Accuracy= 5.50%\n",
            "Iter= 16000, Average Loss= 7.584140, Average Accuracy= 6.60%\n",
            "Iter= 17000, Average Loss= 7.572015, Average Accuracy= 8.10%\n",
            "Iter= 18000, Average Loss= 7.612118, Average Accuracy= 6.50%\n",
            "Iter= 19000, Average Loss= 7.557611, Average Accuracy= 7.30%\n",
            "Iter= 20000, Average Loss= 7.312602, Average Accuracy= 9.10%\n",
            "Iter= 21000, Average Loss= 7.380666, Average Accuracy= 7.90%\n",
            "Iter= 22000, Average Loss= 7.551022, Average Accuracy= 6.60%\n",
            "Iter= 23000, Average Loss= 7.391320, Average Accuracy= 7.00%\n",
            "Iter= 24000, Average Loss= 7.468923, Average Accuracy= 7.50%\n",
            "Iter= 25000, Average Loss= 7.488367, Average Accuracy= 6.80%\n",
            "Iter= 26000, Average Loss= 7.371568, Average Accuracy= 7.90%\n",
            "Iter= 27000, Average Loss= 7.480138, Average Accuracy= 6.50%\n",
            "Iter= 28000, Average Loss= 7.323584, Average Accuracy= 8.40%\n",
            "Iter= 29000, Average Loss= 7.354787, Average Accuracy= 6.30%\n",
            "Iter= 30000, Average Loss= 7.346241, Average Accuracy= 7.70%\n",
            "Iter= 31000, Average Loss= 7.371924, Average Accuracy= 8.00%\n",
            "Iter= 32000, Average Loss= 7.204633, Average Accuracy= 6.80%\n",
            "Iter= 33000, Average Loss= 7.263307, Average Accuracy= 8.70%\n",
            "Iter= 34000, Average Loss= 7.223509, Average Accuracy= 7.90%\n",
            "Iter= 35000, Average Loss= 7.201169, Average Accuracy= 6.40%\n",
            "Iter= 36000, Average Loss= 7.224046, Average Accuracy= 8.90%\n",
            "Iter= 37000, Average Loss= 7.233929, Average Accuracy= 7.60%\n",
            "Iter= 38000, Average Loss= 7.065608, Average Accuracy= 7.10%\n",
            "Iter= 39000, Average Loss= 7.468182, Average Accuracy= 6.30%\n",
            "Iter= 40000, Average Loss= 7.185052, Average Accuracy= 8.40%\n",
            "Optimization Finished!\n",
            "Elapsed time:  56.789608256022134 min\n",
            " <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>\n",
            "Perplexity for LSTM Unigram MOdel= 14978.76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sddWLhXbBGyu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "e8ef0142-1e6a-4609-fe03-6f66523da4f9"
      },
      "source": [
        "print(\"Overehere the Neural network displayed comparatively poor results with respect to classical method. \\nHowever, fine tuning of parameters will result in better results by the Neural netwrok,\\n as they get trained with the context and learn the features directly from the text. Morover, the \")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overehere the Neural network displayed comparatively poor results with respect to classical method. \n",
            "However, fine tuning of parameters will result in better results by the Neural netwrok,\n",
            " as they get trained with the context and learn the features directly from the text. Morover, the \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}